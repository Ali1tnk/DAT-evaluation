#!/usr/bin/env python3
"""
Plot TAPAAL verification results for attack tree diagnosability evaluation.

This script reads results.csv generated by run_tapaal.sh and produces:
1. Bar chart showing SAT vs UNSAT counts
2. Box plot of TAPAAL verification runtimes
3. Summary statistics table

Output files:
- diagnosability_results.png: Combined visualization
- runtime_analysis.png: Detailed runtime analysis
- results_summary.txt: Text summary of results
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from pathlib import Path
import sys


def load_results(results_file: str = "results.csv") -> pd.DataFrame:
    """
    Load TAPAAL verification results from CSV file.
    
    Args:
        results_file: Path to results CSV file
    
    Returns:
        DataFrame with results data
    """
    if not Path(results_file).exists():
        print(f"Error: Results file '{results_file}' not found")
        print("Please run './run_tapaal.sh' first to generate results")
        sys.exit(1)
    
    # Read CSV file
    df = pd.read_csv(results_file)
    
    # Validate expected columns
    expected_columns = ['model', 'result', 'time_sec']
    if not all(col in df.columns for col in expected_columns):
        print(f"Error: Results file missing expected columns: {expected_columns}")
        sys.exit(1)
    
    # Clean up data
    df['time_sec'] = pd.to_numeric(df['time_sec'], errors='coerce')
    
    # Add tree size information if metadata is available
    if Path('tree_metadata.json').exists():
        import json
        with open('tree_metadata.json', 'r') as f:
            metadata = json.load(f)
        
        # Create mapping from tree_id to node count
        tree_sizes = {}
        for tree_info in metadata.get('trees', []):
            tree_id = f"tree_{tree_info['tree_id']:03d}"
            tree_sizes[tree_id] = tree_info['num_nodes']
        
        # Add tree size column
        df['tree_size'] = df['model'].map(tree_sizes)
    
    return df


def create_results_summary(df: pd.DataFrame) -> str:
    """
    Create a text summary of verification results.
    
    Args:
        df: DataFrame with verification results
    
    Returns:
        Formatted summary string
    """
    total_trees = len(df)
    
    # Count results by type
    result_counts = df['result'].value_counts()
    
    # Runtime statistics (excluding timeouts and errors)
    successful_df = df[df['result'].isin(['SAT', 'UNSAT'])]
    
    summary_lines = [
        "TAPAAL Diagnosability Verification Results Summary",
        "=" * 55,
        "",
        f"Total trees analyzed: {total_trees}",
        "",
        "Results breakdown:",
    ]
    
    for result_type in ['SAT', 'UNSAT', 'TIMEOUT', 'ERROR', 'MISSING']:
        count = result_counts.get(result_type, 0)
        percentage = (count / total_trees) * 100 if total_trees > 0 else 0
        summary_lines.append(f"  {result_type:8s}: {count:3d} ({percentage:5.1f}%)")
    
    if len(successful_df) > 0:
        summary_lines.extend([
            "",
            "Runtime statistics (successful verifications only):",
            f"  Mean time:   {successful_df['time_sec'].mean():.3f} seconds",
            f"  Median time: {successful_df['time_sec'].median():.3f} seconds",
            f"  Min time:    {successful_df['time_sec'].min():.3f} seconds",
            f"  Max time:    {successful_df['time_sec'].max():.3f} seconds",
            f"  Std dev:     {successful_df['time_sec'].std():.3f} seconds",
        ])
        
        # Diagnosability analysis
        sat_count = result_counts.get('SAT', 0)
        unsat_count = result_counts.get('UNSAT', 0)
        total_verified = sat_count + unsat_count
        
        if total_verified > 0:
            diagnosable_rate = (sat_count / total_verified) * 100
            summary_lines.extend([
                "",
                "Diagnosability analysis:",
                f"  Trees successfully verified: {total_verified}",
                f"  Diagnosable trees (SAT):     {sat_count} ({diagnosable_rate:.1f}%)",
                f"  Non-diagnosable trees:       {unsat_count} ({100-diagnosable_rate:.1f}%)",
            ])
    
    # Tree size analysis if available
    if 'tree_size' in df.columns:
        size_stats = df.groupby('result')['tree_size'].describe()
        summary_lines.extend([
            "",
            "Tree size analysis:",
        ])
        
        for result_type in ['SAT', 'UNSAT']:
            if result_type in size_stats.index:
                stats = size_stats.loc[result_type]
                summary_lines.append(
                    f"  {result_type} trees - avg size: {stats['mean']:.1f} nodes "
                    f"(range: {stats['min']:.0f}-{stats['max']:.0f})"
                )
    
    summary_lines.extend([
        "",
        "Notes:",
        "- SAT means the tree is weakly diagnosable with the given observation set",
        "- UNSAT means the tree is not diagnosable with the current sensors",
        "- TIMEOUT indicates verification took longer than the time limit",
        "- Observation set: all non-leaf nodes (internal system states)",
    ])
    
    return "\n".join(summary_lines)


def plot_results_overview(df: pd.DataFrame) -> plt.Figure:
    """
    Create overview plots of verification results.
    
    Args:
        df: DataFrame with verification results
    
    Returns:
        matplotlib Figure object
    """
    # Set style
    plt.style.use('default')
    sns.set_palette("husl")
    
    # Create figure with subplots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('TAPAAL Diagnosability Verification Results', fontsize=16, fontweight='bold')
    
    # 1. Bar chart of result counts
    result_counts = df['result'].value_counts()
    colors = {'SAT': 'green', 'UNSAT': 'orange', 'TIMEOUT': 'red', 'ERROR': 'darkred', 'MISSING': 'gray'}
    bar_colors = [colors.get(result, 'blue') for result in result_counts.index]
    
    bars = ax1.bar(result_counts.index, result_counts.values, color=bar_colors, alpha=0.7)
    ax1.set_title('Verification Results Distribution', fontweight='bold')
    ax1.set_ylabel('Number of Trees')
    ax1.set_xlabel('Result Type')
    
    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{int(height)}', ha='center', va='bottom')
    
    # 2. Runtime box plot (excluding timeouts and errors)
    successful_df = df[df['result'].isin(['SAT', 'UNSAT'])]
    if len(successful_df) > 0:
        successful_df.boxplot(column='time_sec', by='result', ax=ax2)
        ax2.set_title('Runtime Distribution by Result Type', fontweight='bold')
        ax2.set_ylabel('Verification Time (seconds)')
        ax2.set_xlabel('Result Type')
        ax2.get_figure().suptitle('')  # Remove default boxplot title
    else:
        ax2.text(0.5, 0.5, 'No successful verifications\nto plot runtimes', 
                ha='center', va='center', transform=ax2.transAxes)
        ax2.set_title('Runtime Distribution (No Data)', fontweight='bold')
    
    # 3. Tree size vs result (if size data available)
    if 'tree_size' in df.columns and df['tree_size'].notna().any():
        size_result_df = df[df['result'].isin(['SAT', 'UNSAT'])]
        if len(size_result_df) > 0:
            for result_type in ['SAT', 'UNSAT']:
                subset = size_result_df[size_result_df['result'] == result_type]
                if len(subset) > 0:
                    ax3.scatter(subset['tree_size'], subset['time_sec'], 
                              label=result_type, alpha=0.6, s=30)
            
            ax3.set_xlabel('Tree Size (Number of Nodes)')
            ax3.set_ylabel('Verification Time (seconds)')
            ax3.set_title('Tree Size vs Verification Time', fontweight='bold')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'No size data available', 
                    ha='center', va='center', transform=ax3.transAxes)
    else:
        ax3.text(0.5, 0.5, 'No tree size data available', 
                ha='center', va='center', transform=ax3.transAxes)
        ax3.set_title('Tree Size Analysis (No Data)', fontweight='bold')
    
    # 4. Runtime histogram
    if len(successful_df) > 0:
        ax4.hist(successful_df['time_sec'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax4.set_xlabel('Verification Time (seconds)')
        ax4.set_ylabel('Number of Trees')
        ax4.set_title('Runtime Distribution Histogram', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # Add vertical line for mean
        mean_time = successful_df['time_sec'].mean()
        ax4.axvline(mean_time, color='red', linestyle='--', alpha=0.8, 
                   label=f'Mean: {mean_time:.3f}s')
        ax4.legend()
    else:
        ax4.text(0.5, 0.5, 'No successful verifications\nto plot histogram', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Runtime Histogram (No Data)', fontweight='bold')
    
    plt.tight_layout()
    return fig


def plot_detailed_runtime_analysis(df: pd.DataFrame) -> plt.Figure:
    """
    Create detailed runtime analysis plots.
    
    Args:
        df: DataFrame with verification results
    
    Returns:
        matplotlib Figure object
    """
    successful_df = df[df['result'].isin(['SAT', 'UNSAT'])]
    
    if len(successful_df) == 0:
        # Create empty figure with message
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        ax.text(0.5, 0.5, 'No successful verifications for detailed runtime analysis', 
               ha='center', va='center', transform=ax.transAxes, fontsize=14)
        ax.set_title('Detailed Runtime Analysis (No Data)', fontweight='bold')
        return fig
    
    # Create figure with subplots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Detailed Runtime Analysis', fontsize=16, fontweight='bold')
    
    # 1. Cumulative runtime plot
    sorted_times = successful_df['time_sec'].sort_values()
    cumulative_times = np.cumsum(sorted_times)
    ax1.plot(range(1, len(cumulative_times) + 1), cumulative_times, 'b-', linewidth=2)
    ax1.set_xlabel('Tree Number (sorted by runtime)')
    ax1.set_ylabel('Cumulative Time (seconds)')
    ax1.set_title('Cumulative Verification Time', fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # 2. Runtime vs tree ID
    tree_ids = successful_df['model'].str.extract(r'tree_(\d+)')[0].astype(int)
    successful_df_with_id = successful_df.copy()
    successful_df_with_id['tree_id'] = tree_ids
    successful_df_with_id = successful_df_with_id.sort_values('tree_id')
    
    colors = ['green' if r == 'SAT' else 'orange' for r in successful_df_with_id['result']]
    ax2.scatter(successful_df_with_id['tree_id'], successful_df_with_id['time_sec'], 
               c=colors, alpha=0.6, s=30)
    ax2.set_xlabel('Tree ID')
    ax2.set_ylabel('Verification Time (seconds)')
    ax2.set_title('Runtime vs Tree ID', fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # Custom legend
    import matplotlib.patches as mpatches
    sat_patch = mpatches.Patch(color='green', label='SAT (Diagnosable)')
    unsat_patch = mpatches.Patch(color='orange', label='UNSAT (Non-diagnosable)')
    ax2.legend(handles=[sat_patch, unsat_patch])
    
    # 3. Runtime percentiles
    percentiles = [50, 75, 90, 95, 99]
    percentile_values = [np.percentile(successful_df['time_sec'], p) for p in percentiles]
    
    ax3.bar(range(len(percentiles)), percentile_values, color='lightcoral', alpha=0.7)
    ax3.set_xticks(range(len(percentiles)))
    ax3.set_xticklabels([f'{p}th' for p in percentiles])
    ax3.set_ylabel('Verification Time (seconds)')
    ax3.set_xlabel('Percentile')
    ax3.set_title('Runtime Percentiles', fontweight='bold')
    
    # Add value labels
    for i, val in enumerate(percentile_values):
        ax3.text(i, val + max(percentile_values)*0.01, f'{val:.3f}s', 
                ha='center', va='bottom')
    
    # 4. Tree size impact (if available)
    if 'tree_size' in successful_df.columns and successful_df['tree_size'].notna().any():
        # Group by tree size and show average runtime
        size_groups = successful_df.groupby('tree_size')['time_sec'].agg(['mean', 'std', 'count'])
        size_groups = size_groups.dropna()
        
        if len(size_groups) > 1:
            ax4.errorbar(size_groups.index, size_groups['mean'], 
                        yerr=size_groups['std'], fmt='o-', capsize=5, capthick=2)
            ax4.set_xlabel('Tree Size (Number of Nodes)')
            ax4.set_ylabel('Average Verification Time (seconds)')
            ax4.set_title('Average Runtime by Tree Size', fontweight='bold')
            ax4.grid(True, alpha=0.3)
            
            # Add count annotations
            for size, row in size_groups.iterrows():
                ax4.annotate(f'n={int(row["count"])}', 
                           (size, row['mean']), 
                           textcoords="offset points", xytext=(0,10), ha='center')
        else:
            ax4.text(0.5, 0.5, 'Insufficient tree size variation', 
                    ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Tree Size Analysis (Insufficient Data)', fontweight='bold')
    else:
        ax4.text(0.5, 0.5, 'No tree size data available', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Tree Size Analysis (No Data)', fontweight='bold')
    
    plt.tight_layout()
    return fig


def main():
    """Main function to generate all result plots and summaries."""
    print("Analyzing TAPAAL verification results...")
    
    # Load results
    df = load_results()
    print(f"Loaded {len(df)} verification results")
    
def main():
    """Main function to generate all result plots and summaries."""
    print("Analyzing TAPAAL verification results...")
    
    # Load results
    df = load_results()
    print(f"Loaded {len(df)} verification results")
    
    # Generate text summary
    summary = create_results_summary(df)
    print("\n" + summary)
    
    # Save summary to file
    with open('results_summary.txt', 'w') as f:
        f.write(summary)
    print(f"\nSummary saved to: results_summary.txt")
    
    # Generate overview plots
    print("Generating overview plots...")
    fig1 = plot_results_overview(df)
    fig1.savefig('diagnosability_results.png', dpi=300, bbox_inches='tight')
    print("Overview plots saved to: diagnosability_results.png")
    
    # Generate detailed runtime analysis
    print("Generating detailed runtime analysis...")
    fig2 = plot_detailed_runtime_analysis(df)
    fig2.savefig('runtime_analysis.png', dpi=300, bbox_inches='tight')
    print("Runtime analysis saved to: runtime_analysis.png")
    
    # Show plots if in interactive mode
    try:
        # Check if we're in an interactive environment
        import matplotlib
        if matplotlib.get_backend() != 'Agg':
            plt.show()
    except:
        pass
    
    print("\nAnalysis complete!")
    print("Generated files:")
    print("- results_summary.txt: Text summary of all results")
    print("- diagnosability_results.png: Overview visualization")
    print("- runtime_analysis.png: Detailed runtime analysis")


if __name__ == "__main__":
    main()
